{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migração do Spark para o BigQuery via Dataproc - Parte 3\n",
    "\n",
    "*  [Parte 1](01_spark - Laboratorio Google.ipynb): O código Spark original, agora em execução no Dataproc (lift-and-shift)\n",
    "*  [Parte 2](02_gcs - Laboratorio Google.ipynb): Substitua o HDFS pelo Google Cloud Storage. Isso permite clusters específicos de trabalho. (nativo da nuvem)\n",
    "*  [Parte 3](03_automate - Laboratorio Google.ipynb): Automatize tudo, para que possamos executar em um cluster específico de trabalho. (otimizado para nuvem)\n",
    "*  [Parte 4](04_bigquery - Laboratorio Google.ipynb): Carregue CSV no BigQuery, use o BigQuery. (modernizar)\n",
    "*  [Parte 5](05_functions - Laboratorio Google.ipynb): Usando o Cloud Functions, inicie a análise sempre que houver um novo arquivo no intervalo. (sem servidor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catch up: dados para GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catch up cell. Run if you did not do previous notebooks of this sequence\n",
    "!wget http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\n",
    "BUCKET='cloud-training-demos-ml'  # CHANGE\n",
    "!pip install google-compute-engine\n",
    "!gsutil cp kdd* gs://$BUCKET/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET='cloud-training-demos-ml'  # CHANGE\n",
    "!gsutil ls gs://$BUCKET/kdd*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crie um arquivo Python\n",
    "Coloque todo o código em um arquivo Python. Podemos comentar o código de exibição apenas como ```take()``` e ```show()```\n",
    "Faça configurações alteráveis como ```BUCKET``` vindo de sys.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile spark_analysis.py\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--bucket\", help=\"bucket for input and output\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "BUCKET = args.bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"kdd\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "data_file = \"gs://{}/kddcup.data_10_percent.gz\".format(BUCKET)\n",
    "raw_rdd = sc.textFile(data_file).cache()\n",
    "#raw_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "\n",
    "csv_rdd = raw_rdd.map(lambda row: row.split(\",\"))\n",
    "parsed_rdd = csv_rdd.map(lambda r: Row(\n",
    "    duration=int(r[0]), \n",
    "    protocol_type=r[1],\n",
    "    service=r[2],\n",
    "    flag=r[3],\n",
    "    src_bytes=int(r[4]),\n",
    "    dst_bytes=int(r[5]),\n",
    "    wrong_fragment=int(r[7]),\n",
    "    urgent=int(r[8]),\n",
    "    hot=int(r[9]),\n",
    "    num_failed_logins=int(r[10]),\n",
    "    num_compromised=int(r[12]),\n",
    "    su_attempted=r[14],\n",
    "    num_root=int(r[15]),\n",
    "    num_file_creations=int(r[16]),\n",
    "    label=r[-1]\n",
    "    )\n",
    ")\n",
    "#parsed_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.createDataFrame(parsed_rdd)\n",
    "connections_by_protocol = df.groupBy('protocol_type').count().orderBy('count', ascending=False)\n",
    "connections_by_protocol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "\n",
    "df.registerTempTable(\"connections\")\n",
    "attack_stats = sqlContext.sql(\"\"\"\n",
    "                           SELECT \n",
    "                             protocol_type, \n",
    "                             CASE label\n",
    "                               WHEN 'normal.' THEN 'no attack'\n",
    "                               ELSE 'attack'\n",
    "                             END AS state,\n",
    "                             COUNT(*) as total_freq,\n",
    "                             ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n",
    "                             ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n",
    "                             ROUND(AVG(duration), 2) as mean_duration,\n",
    "                             SUM(num_failed_logins) as total_failed_logins,\n",
    "                             SUM(num_compromised) as total_compromised,\n",
    "                             SUM(num_file_creations) as total_file_creations,\n",
    "                             SUM(su_attempted) as total_root_attempts,\n",
    "                             SUM(num_root) as total_root_acceses\n",
    "                           FROM connections\n",
    "                           GROUP BY protocol_type, state\n",
    "                           ORDER BY 3 DESC\n",
    "                           \"\"\")\n",
    "attack_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "\n",
    "ax = attack_stats.toPandas().plot.bar(x='protocol_type', subplots=True, figsize=(10,25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escrever relatório\n",
    "\n",
    "Certifique-se de copiar a saída para o GCS para que possamos excluir o cluster com segurança. Deve ser Python puro, portanto, substitua os comandos do shell por código Python equivalente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "\n",
    "ax[0].get_figure().savefig('report.png');\n",
    "#!gsutil rm -rf gs://$BUCKET/sparktobq/\n",
    "#!gsutil cp report.png gs://$BUCKET/sparktobq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "\n",
    "import google.cloud.storage as gcs\n",
    "bucket = gcs.Client().get_bucket(BUCKET)\n",
    "for blob in bucket.list_blobs(prefix='sparktobq/'):\n",
    "    blob.delete()\n",
    "bucket.blob('sparktobq/report.png').upload_from_filename('report.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "\n",
    "connections_by_protocol.write.format(\"csv\").mode(\"overwrite\").save(\n",
    "    \"gs://{}/sparktobq/connections_by_protocol\".format(BUCKET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automação de teste\n",
    "\n",
    "Execute-o de forma autônoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET='cloud-training-demos-ml'  # CHANGE\n",
    "print('Writing to {}'.format(BUCKET))\n",
    "!python spark_analysis.py --bucket=$BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://$BUCKET/sparktobq/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
